{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "import spacy\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import  PorterStemmer, WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class G2ProductFeatureList():\n",
    "    \n",
    "    def __init__(self,token):\n",
    "        self.review_dict = {}\n",
    "        self.likes_dict = {}\n",
    "        self.nlp_model = spacy.load(\"en_core_web_lg\")\n",
    "        self.token = token\n",
    "        self.feature_dict = {}\n",
    "        self.clusters = {}\n",
    "        self.top_features_per_cluster = {}\n",
    "        \n",
    "    \n",
    "    \n",
    "    def fetch_reviews(self, number_of_reviews=10, page_num=1):\n",
    "        url = \"https://data.g2.com/api/v1/survey-responses\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Token token={self.token}\",\n",
    "            \"Content-Type\": \"application/vnd.api+json\"\n",
    "        }        \n",
    "        params = {\n",
    "            \"page[size]\": f\"{number_of_reviews}\",\n",
    "            \"page[number]\": f\"{page_num}\"  \n",
    "        }\n",
    "\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            return data['data']\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} - {response.text}\")\n",
    "            return None\n",
    "    \n",
    "    \n",
    "    def change_to_dd_mm_yyyy(self,rfc3339_date):\n",
    "        dt_object = datetime.fromisoformat(rfc3339_date)\n",
    "        return dt_object.strftime('%d%m%Y')\n",
    "\n",
    "\n",
    "    def add_to_review_dict(self,review):\n",
    "            \n",
    "        attributes = review[\"attributes\"]\n",
    "        review_id = review[\"id\"]\n",
    "        try:\n",
    "            self.review_dict[review_id] = {\n",
    "                \"ease_of_doing_business_with\": attributes[\"secondary_answers\"].get(\"ease_of_doing_business_with\", {}).get(\"value\"),\n",
    "                \"country_name\": attributes[\"country_name\"],\n",
    "                \"votes_up\": attributes[\"votes_up\"],\n",
    "                \"votes_down\": attributes[\"votes_down\"],\n",
    "                \"votes_total\": attributes[\"votes_total\"],\n",
    "                \"star_rating\": attributes[\"star_rating\"],\n",
    "                \"time\": self.change_to_dd_mm_yyyy(attributes[\"submitted_at\"]),\n",
    "                \"likes\": attributes[\"comment_answers\"][\"love\"][\"value\"],\n",
    "                \"dislikes\": attributes[\"comment_answers\"][\"hate\"][\"value\"]\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing review: {e}\")\n",
    "    \n",
    "    def create_review_dictionary(self,num_of_reviews,page ):\n",
    "   \n",
    "        if type(page) == int:\n",
    "            reviews = self.fetch_reviews(num_of_reviews,page)\n",
    "            for review in reviews:\n",
    "                self.add_to_review_dict(review)\n",
    "        else:\n",
    "            for i in page:\n",
    "                reviews = self.fetch_reviews(num_of_reviews,page)\n",
    "                for review in reviews:\n",
    "                    self.add_to_review_dict(review)\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "    \n",
    "    def change_name(self,sentence):\n",
    "        nlp = self.nlp_model\n",
    "        doc = nlp(sentence)\n",
    "        replaced_sentence = sentence\n",
    "        for entity in doc.ents:\n",
    "            if entity.label_ == \"PERSON\":\n",
    "                replaced_sentence = replaced_sentence.replace(entity.text, \"person\")\n",
    "            if entity.label_ == \"ORG\":\n",
    "                replaced_sentence = replaced_sentence.replace(entity.text, \"organization\")            \n",
    "        return replaced_sentence\n",
    "\n",
    "\n",
    "    def create_likes_dictionary(self):\n",
    "        for review_id, review in self.review_dict.items():\n",
    "            likes = review[\"likes\"]\n",
    "            if likes in self.likes_dict:\n",
    "                self.likes_dict[review_id].append(likes)\n",
    "            else:\n",
    "                self.likes_dict[review_id] = [likes]\n",
    "                \n",
    "                  \n",
    "    \n",
    "    def extract_all_features(self, show = False):\n",
    "        json_file_path = \"feature_dict.json\"\n",
    "\n",
    "        with open(json_file_path, \"r\") as json_file:\n",
    "            self.feature_dict = json.load(json_file)\n",
    "            \n",
    "        if show:\n",
    "            for review_id, review in self.feature_dict.items():\n",
    "                print(f\"Review ID: {review_id}\")\n",
    "                for i, feature in enumerate(review):\n",
    "                    print(f\"Feature {i + 1}: {feature['generated_text']}\")\n",
    "                print() \n",
    "            \n",
    "    \n",
    "    def preprocess_features(self):\n",
    "        for review_id in self.feature_dict:\n",
    "            review = self.feature_dict[review_id][0][\"generated_text\"].lower()\n",
    "            if review.find(\"feature1, feature2, feature3, ...\") != -1:\n",
    "                self.feature_dict[review_id]= review[review.find(\"feature1, feature2, feature3, ...\")+len(\"feature1, feature2, feature3, ...\"):].strip()\n",
    "                \n",
    "        for review_id in self.feature_dict:\n",
    "            review = self.feature_dict[review_id]\n",
    "            if review.find(\"features:\") != -1:\n",
    "                self.feature_dict[review_id] = review[review.find(\"features:\")+len(\"features:\"):].strip()\n",
    "                \n",
    "        for review_id in self.feature_dict:\n",
    "            # remove punctuations and escape sequences from the string and remove digits\n",
    "            review = self.feature_dict[review_id]\n",
    "            review = review.replace(\"\\n\", \"\")\n",
    "            review = review.replace(\"\\r\", \"\")\n",
    "            review = review.replace(\".\", \"\")\n",
    "            review = review.replace(\"\\\"\", \"\")\n",
    "            review = review.replace(\"\\'\", \"\")\n",
    "            review = review.replace(\"/\", \"\")\n",
    "            review = review.replace(\"(\", \"\")\n",
    "            review = review.replace(\")\", \"\")\n",
    "            review = review.replace(\"*\", \"\")\n",
    "            review = review.replace(\"-\", \"\")\n",
    "            review = review.replace(\":\", \"\")\n",
    "            \n",
    "            for letter in review:\n",
    "                if letter.isdigit():\n",
    "                    review = review.replace(letter, \"\")\n",
    "            \n",
    "            self.feature_dict[review_id] = review\n",
    "    \n",
    "        for review_id in self.feature_dict:\n",
    "            review = self.feature_dict[review_id].strip()\n",
    "            review = review.split(\",\")\n",
    "            \n",
    "            sorted_review = sorted(review, key = len)\n",
    "        \n",
    "        \n",
    "            for i in range(len(sorted_review)):\n",
    "                feature = sorted_review[i].strip()\n",
    "                \n",
    "                if len(feature.split(\" \")) > 3 or len(feature.split(\" \")) < 2:\n",
    "                    feature = \"\"\n",
    "                \n",
    "                sorted_review[i] = feature\n",
    "        \n",
    "            self.feature_dict[review_id] = sorted_review\n",
    "            \n",
    "        for review_id in self.feature_dict:\n",
    "            features = self.feature_dict[review_id]\n",
    "            if features == [\"\"]:\n",
    "                self.feature_dict[review_id] = []\n",
    "                continue\n",
    "            \n",
    "            # Create a new list to store the non-empty features\n",
    "            cleaned_features = []\n",
    "            \n",
    "            for feature in features:\n",
    "                if feature != \"\":\n",
    "                    cleaned_features.append(feature)\n",
    "            \n",
    "            # Update the features in the dictionary\n",
    "            self.feature_dict[review_id] = cleaned_features\n",
    "            \n",
    "    def create_corpus(self):\n",
    "        corpus = []\n",
    "        \n",
    "        for review_id in self.feature_dict:\n",
    "            corpus.append(self.feature_dict[review_id])\n",
    "            \n",
    "        return corpus\n",
    "    \n",
    "    # Function for text preprocessing\n",
    "    def preprocess_text(self,phrase):\n",
    "        # Tokenization\n",
    "        res = []\n",
    "        for i in phrase:\n",
    "            \n",
    "            tokens = word_tokenize(i)\n",
    "            # stemmer = PorterStemmer()\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            # Convert to lowercase\n",
    "            tokens = [word.lower() for word in tokens]\n",
    "            \n",
    "            # Remove punctuation\n",
    "            table = str.maketrans('', '', string.punctuation)\n",
    "            tokens = [word.translate(table) for word in tokens]\n",
    "            \n",
    "            # Remove stopwords\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            \n",
    "            # Stem tokens\n",
    "            #tokens = [stemmer.stem(token) for token in tokens]\n",
    "            tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "            \n",
    "            tokens = [word for word in tokens if word not in stop_words ]\n",
    "            tokens = [word for word in tokens if not word.isdigit()]\n",
    "\n",
    "            \n",
    "            tokens= \" \".join(tokens)\n",
    "                    \n",
    "            res.append(tokens)\n",
    "        return res\n",
    "    \n",
    "    def preprocess_corpus(self):\n",
    "        corpus = self.create_corpus()\n",
    "        preprocessed_corpus = []\n",
    "        # Preprocess each document in the corpus\n",
    "        for i in range(len(corpus)):\n",
    "            preprocessed_corpus.append(self.preprocess_text(corpus[i]))\n",
    "        return preprocessed_corpus\n",
    "\n",
    "    def cluster_creation(self ,few_datapoints = False ,show= False):\n",
    "        preprocessed_dataset = self.preprocess_corpus()\n",
    "        flattened_dataset = [item for sublist in preprocessed_dataset for item in sublist]\n",
    "        \n",
    "        # TF-IDF Vectorization\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform(flattened_dataset)\n",
    "        \n",
    "        # Calculate pairwise cosine similarity\n",
    "        cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "        \n",
    "        # K-means clustering\n",
    "        k = 54  # Number of clusters (adjust as needed)\n",
    "        if few_datapoints:\n",
    "            k = 45\n",
    "        kmeans = KMeans(n_clusters=k)\n",
    "        kmeans.fit(cosine_sim)\n",
    "        \n",
    "        # Extracting important features from each cluster\n",
    "        \n",
    "        for i, label in enumerate(kmeans.labels_):\n",
    "            if label not in self.clusters:\n",
    "                self.clusters[label] = [flattened_dataset[i]]\n",
    "            else:\n",
    "                self.clusters[label].append(flattened_dataset[i])\n",
    "                \n",
    "        if show:\n",
    "            # Print out important features in each cluster\n",
    "            for cluster_id, features in self.clusters.items():\n",
    "                print(f\"Cluster {cluster_id}:\")\n",
    "                for feature in features:\n",
    "                    print(f\"- {feature}\")\n",
    "                print()\n",
    "                \n",
    "    \n",
    "    def find_word_freq(self,cluster):\n",
    "        all_features = [feature for features in cluster for feature in features.split()]\n",
    "        term_frequency = Counter(all_features)\n",
    "        return term_frequency\n",
    "    \n",
    "    def feature_importance(self,show=False):\n",
    "        cluster_frequency = {}\n",
    "        for cluster_id, cluster in self.clusters.items():\n",
    "            cluster_frequency[cluster_id] = self.find_word_freq(cluster)\n",
    "        \n",
    "                # Update clusters dictionary to include feature importance\n",
    "        for cluster_id, cluster in self.clusters.items():\n",
    "            self.clusters[cluster_id] = []\n",
    "            for feature in cluster:\n",
    "                features = feature.split()\n",
    "                feature_importance = sum(cluster_frequency[cluster_id].get(word, 0) for word in features)\n",
    "                self.clusters[cluster_id].append({\"feature\": feature, \"importance\": feature_importance})\n",
    "                \n",
    "        sorted_clusters = sorted(self.clusters.items(), key=lambda x: sum(feature[\"importance\"] for feature in x[1]), reverse=True)\n",
    "        \n",
    "        # Sort features within each cluster based on their importance\n",
    "        for cluster_id, cluster_features in sorted_clusters:\n",
    "            sorted_features = sorted(cluster_features, key=lambda x: x[\"importance\"], reverse=True)\n",
    "            self.clusters[cluster_id] = sorted_features\n",
    "            \n",
    "        if show:\n",
    "            # Print sorted clusters\n",
    "            for cluster_id, cluster_features in sorted_clusters:\n",
    "                print(f\"Cluster {cluster_id}:\")\n",
    "                for feature in cluster_features:\n",
    "                    print(f\"- {feature['feature']} (Importance: {feature['importance']})\")\n",
    "                print()\n",
    "                \n",
    "                \n",
    "    def process_clusters(self,show = False):\n",
    "        # Remove duplicates within each cluster\n",
    "        for cluster_id, cluster_features in self.clusters.items():\n",
    "            unique_features = []\n",
    "            seen_features = set()\n",
    "            for feature in cluster_features:\n",
    "                if feature[\"feature\"] not in seen_features:\n",
    "                    unique_features.append(feature)\n",
    "                    seen_features.add(feature[\"feature\"])\n",
    "            self.clusters[cluster_id] = unique_features\n",
    "\n",
    "        # Sort clusters based on the total importance\n",
    "        sorted_clusters = sorted(self.clusters.items(), key=lambda x: sum(feature[\"importance\"] for feature in x[1]), reverse=True)\n",
    "\n",
    "        # Sort features within each cluster based on their importance\n",
    "        for cluster_id, cluster_features in sorted_clusters:\n",
    "            sorted_features = sorted(cluster_features, key=lambda x: x[\"importance\"], reverse=True)\n",
    "            self.clusters[cluster_id] = sorted_features\n",
    "\n",
    "        if show:\n",
    "            # Print sorted clusters with duplicates removed\n",
    "            for cluster_id, cluster_features in sorted_clusters:\n",
    "                print(f\"Cluster {cluster_id}:\")\n",
    "                for feature in cluster_features:\n",
    "                    print(f\"- {feature['feature']} (Importance: {feature['importance']})\")\n",
    "                print()\n",
    "\n",
    "                \n",
    "    def pick_top_k_features(self,k=2 , show = False):\n",
    "        # Pick top 2 features from each cluster\n",
    "        top_features_per_cluster = {}\n",
    "\n",
    "        for cluster_id, cluster_features in self.clusters.items():\n",
    "            sorted_features = sorted(cluster_features, key=lambda x: x[\"importance\"], reverse=True)\n",
    "            top_features_per_cluster[cluster_id] = sorted_features[:6]\n",
    "\n",
    "        if show:\n",
    "            # Print top 2 features from each cluster\n",
    "            for cluster_id, top_features in top_features_per_cluster.items():\n",
    "                print(f\"Cluster {cluster_id}:\")\n",
    "                for feature in top_features:\n",
    "                    print(f\"- {feature['feature']} (Importance: {feature['importance']})\")\n",
    "                print()\n",
    "                \n",
    "    \n",
    "        self.all_top_features = [feature for  top_features in top_features_per_cluster.values() for feature in top_features]\n",
    "\n",
    "        # Sort the list of top features based on importance scores\n",
    "        self.all_top_features.sort(key=lambda x: x['importance'], reverse=True)\n",
    "\n",
    "        all_features = []\n",
    "        \n",
    "        if show :\n",
    "            # Print the sorted list of top features\n",
    "            for feature in self.all_top_features:\n",
    "                all_features.append(feature['feature'])\n",
    "                if show:\n",
    "                    print(f\"- {feature['feature']}\")\n",
    "                \n",
    "    def get_wordcloud(self):\n",
    "        \n",
    "\n",
    "        # Create a string with all features and their importance scores\n",
    "        feature_text = ' '.join([f\"{feature['feature']} \" * feature['importance'] for feature in self.all_top_features])\n",
    "\n",
    "        # Generate the word cloud\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(feature_text)\n",
    "\n",
    "        # Display the word cloud\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "            \n",
    "            \n",
    "    def run(self):\n",
    "        self.create_review_dictionary(num_of_reviews=100,page_num=1)\n",
    "        self.create_likes_dictionary()\n",
    "        self.extract_all_features()\n",
    "        self.preprocess_features()\n",
    "        self.cluster_creation()\n",
    "        self.feature_importance()\n",
    "        self.process_clusters()\n",
    "        self.pick_top_k_features()\n",
    "        self.get_wordcloud()\n",
    "        \n",
    "        return self.all_top_features   \n",
    "    \n",
    "    def geographic_features(self, country_name,page, show=False):\n",
    "        try:\n",
    "            \n",
    "            self.create_review_dictionary(num_of_reviews=100,page=page)\n",
    "            # Extract features for all reviews\n",
    "            self.extract_all_features(show=show)\n",
    "            \n",
    "            # Filter reviews based on country_name\n",
    "            for review_id in list(self.review_dict.keys()):\n",
    "                if self.review_dict[review_id][\"country_name\"] != country_name:\n",
    "                    del self.review_dict[review_id]\n",
    "            \n",
    "            # Preprocess features for the filtered reviews\n",
    "            self.preprocess_features()\n",
    "            \n",
    "            # Perform clustering on the filtered reviews\n",
    "            self.cluster_creation(show=show)\n",
    "            \n",
    "            # Analyze feature importance\n",
    "            self.feature_importance(show=show)\n",
    "            \n",
    "            # Process clusters\n",
    "            self.process_clusters(show=show)\n",
    "            \n",
    "            # Pick top k features\n",
    "            self.pick_top_k_features(show=show)\n",
    "            \n",
    "            # Generate word cloud\n",
    "            self.get_wordcloud()    \n",
    "        except KeyError:\n",
    "            print(f\"No reviews found for the country: {country_name}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "token = os.getenv(\"API_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 400 - {\"errors\":[{\"title\":\"Invalid page value\",\"detail\":\"0 is not a valid value for number page parameter.\",\"code\":\"118\",\"status\":\"400\"}]}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m g2 \u001b[38;5;241m=\u001b[39m G2ProductFeatureList(token)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mg2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeographic_features\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIndia\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[69], line 403\u001b[0m, in \u001b[0;36mG2ProductFeatureList.geographic_features\u001b[1;34m(self, country_name, page, show)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgeographic_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, country_name,page, show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_review_dictionary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_of_reviews\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mpage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;66;03m# Extract features for all reviews\u001b[39;00m\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_all_features(show\u001b[38;5;241m=\u001b[39mshow)\n",
      "Cell \u001b[1;32mIn[69], line 87\u001b[0m, in \u001b[0;36mG2ProductFeatureList.create_review_dictionary\u001b[1;34m(self, num_of_reviews, page)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m page:\n\u001b[0;32m     86\u001b[0m     reviews \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfetch_reviews(num_of_reviews,page)\n\u001b[1;32m---> 87\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreview\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreviews\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_to_review_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreview\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "\n",
    "g2 = G2ProductFeatureList(token)\n",
    "g2.geographic_features(\"India\",[1,2,3,4,5,6,7] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'184054': {'ease_of_doing_business_with': 7.0,\n",
       "  'country_name': 'India',\n",
       "  'votes_up': 0,\n",
       "  'votes_down': 0,\n",
       "  'votes_total': 0,\n",
       "  'star_rating': 5.0,\n",
       "  'time': '25102016',\n",
       "  'likes': 'Being in Customer Success, it is extremely important to know how your customers feel about your product and service and getting constant feedback is absolutely necessary. With Advocately, we get an all-in-one platform that gives us the data we need to help us engage better with our clients.',\n",
       "  'dislikes': \"I really can't think of anything right now.\"},\n",
       " '201553': {'ease_of_doing_business_with': None,\n",
       "  'country_name': 'India',\n",
       "  'votes_up': 0,\n",
       "  'votes_down': 0,\n",
       "  'votes_total': 0,\n",
       "  'star_rating': 5.0,\n",
       "  'time': '30112016',\n",
       "  'likes': 'Advocately is very easy to use and we’ve received outstanding support from their team. It helped us understand our customer’s requirements better. We were able to see results in just a few months! ',\n",
       "  'dislikes': \"Haven't had a bad experience so far.  :)\"},\n",
       " '288758': {'ease_of_doing_business_with': 7.0,\n",
       "  'country_name': 'India',\n",
       "  'votes_up': 0,\n",
       "  'votes_down': 0,\n",
       "  'votes_total': 0,\n",
       "  'star_rating': 5.0,\n",
       "  'time': '09052017',\n",
       "  'likes': 'Easy to setup, Best Support and good pricing!  ',\n",
       "  'dislikes': 'Nothing as such. Started using this tool a month and feeling really good about the way it has brought in reviews. '},\n",
       " '492019': {'ease_of_doing_business_with': 7.0,\n",
       "  'country_name': 'India',\n",
       "  'votes_up': 0,\n",
       "  'votes_down': 0,\n",
       "  'votes_total': 0,\n",
       "  'star_rating': 5.0,\n",
       "  'time': '18122017',\n",
       "  'likes': '1.  Strong focus on review management.\\r\\n2.  Well thought-out process for maintaining authenticity.\\r\\n3.  Value-driven marketing platform which enhances PR and social proof. \\r\\n4. Strong support from the G2 crowd team.',\n",
       "  'dislikes': 'It can seem like there is a lot happening in the platform initially which can make self-implementation a cumbersome process. '}}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g2.review_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
